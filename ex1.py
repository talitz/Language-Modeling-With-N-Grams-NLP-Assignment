from collections import Counter
import collections 
import re
import random

class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns amodel from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and caracter level.
    """

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Arges:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """ 
        self.n = n
        self.chars = chars
        self.ngrams_dictionaries = {}

    def build_model(self, text):  #should be called build_model
        """populates a dictionary counting all ngrams in the specified text.

            Args:
                text (str): the text to construct the model from.
        """
        # Loop through all lines and words and add n-grams to dict
        tokens = [token for token in text.split(" ") if token != ""]
        self.text = text
        for j in range(1,self.n+1):
            # Use the zip function to help us generate n-grams
            # Concatentate the tokens into ngrams and return
            ngrams_dictionary = []
            ngrams = zip(*[tokens[i:] for i in range(j)])
            ngrams = (' '.join(ngram) for ngram in ngrams)
            ngrams_count = collections.defaultdict(int)
            for ngram in ngrams:
                ngrams_count[ngram] += 1
            ngrams_dictionary = ngrams_count
            self.ngrams_dictionaries[j] = ngrams_dictionary
        
    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
        """
        return self.ngrams_dictionaries[self.n]

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        current_ngram, full_output_sentence = [], []
        
        if(context is None):
            #print("Choosing random n-gram = {}".format(current_ngram))
            current_ngram = random.choice(list(self.ngrams_dictionaries[self.n].keys()))
        else:    
            current_ngram = context
     
        current_ngram_length = len(current_ngram.split(' '))
        potential_ngrams = []
        i = 0
        full_output_sentence.append(current_ngram)
        
        if(current_ngram_length < self.n-1):
            #print("context given was size of = {}".format(current_ngram_length))
            for s in range(current_ngram_length+1,self.n):
                #iterating over the language, looking for all possibles ngrams to complete
                for key, value in self.ngrams_dictionaries[s].items():
                    if(isinstance(key, str) and key.startswith(current_ngram)):
                        #print("n-gram {} can potentially complete our sentence!".format(key))
                        for l in range(1,value+1):
                            potential_ngrams.append(key)                        
                        #print("Potential ngrams = {}".format(potential_ngrams))
                    #else:
                        #print("n-gram {} can NOT complete our sentence!".format(key))
            #print("Potential ngrams = {}".format(potential_ngrams))
            if not potential_ngrams:
                return ' '.join(full_output_sentence)
            else: 
                chosen_ngram = ' '.join(random.choices(potential_ngrams))            
                #print("Appending the completed ngram that was found to our sentence = {}".format(chosen_ngram))
                full_output_sentence.append(chosen_ngram.split(' ')[-1])
                #print("Full Sentence Output now is = {}".format(full_output_sentence))
                current_ngram = chosen_ngram
            
        i = self.n
        #print("Full Sentence Output = {}".format(full_output_sentence))
            
        #iterate until we get to n words total, including the first self.n words we found already
        while i < n-6:
            #calculating the new gram to be completed
            current_ngram = current_ngram.split(' ')
            current_ngram.reverse()
            new_ngram_to_complete = []
            for a in range(0,self.n-1):
                new_ngram_to_complete.append(current_ngram[a])

            new_ngram_to_complete.reverse()
            new_ngram_to_complete_string = ' '.join(new_ngram_to_complete)
            #print("New ngram to complete now is = {}".format(new_ngram_to_complete_string))
            potential_ngrams = []

            #iterating over the language, looking for all possibles ngrams to complete
            for key, value in self.ngrams_dictionaries[self.n].items():
                if(isinstance(key, str) and key.startswith(new_ngram_to_complete_string)):
                    #print("n-gram {} can potentially complete our sentence!".format(key))
                    for l in range(1,value+1):
                        potential_ngrams.append(key)                        
                    #print("Potential ngrams = {}".format(potential_ngrams))
                #else:
                    #print("n-gram {} can NOT complete our sentence!".format(key))
            #print("Potential ngrams = {}".format(potential_ngrams))
            if not potential_ngrams:
                return ' '.join(full_output_sentence)
            else: 
                chosen_ngram = ' '.join(random.choices(potential_ngrams))            
                #print("Appending the completed ngram that was found to our sentence = {}".format(chosen_ngram))
                full_output_sentence.append(chosen_ngram.split(' ')[-1])
                #print("Full Sentence Output now is = {}".format(full_output_sentence))
                i = i + self.n
                current_ngram = chosen_ngram
        return ' '.join(full_output_sentence)
            
            
    def evaluate(self,text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to ebaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """

def normalize_text(text):
    """Returns a normalized string based on the specifiy string.
       You can add default parameters as you like (they should have default values!)
       You should explain your decitions in the header of the function.

       Args:
           text (str): the text to normalize

       Returns:
           string. the normalized text.
    """
    text = re.sub('([.,!?()])', r' \1 ', text)
    text = re.sub('\s{2,}', ' ', text)
    return text.lower()

def who_am_i():
    """Returns a ductionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return {'name': 'Tal Yitzhak', 'id': '204260533', 'email': 'talyitzhak100@gmail.com'}
